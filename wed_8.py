# -*- coding: utf-8 -*-
"""wed_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1itNiQ2MYC4ho4XwZUuBTITt1-3O7840C
"""

# function for multiple pdfs  -- done
# function for websites -- done

"""## With multiple PDFs"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import nltk
# nltk.download('punkt')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install git+https://github.com/neuml/txtai

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install txtai[pipeline]

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install Pillow==9.0.0

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from txtai.pipeline import Textractor

num_pdf = int(input("Enter number of files - "))

locations_max = []
for i in range (num_pdf):
  location = input("Enter the path - ")
  locations_max.append(location)
print(locations_max)

textract = Textractor(sentences=True)

data_lines = []
for i in (locations_max):
  print(i)
  print()
  lines = textract(i)
  data_lines.append(lines)

len(data_lines)

len(data_lines[0])

len(data_lines[1])

from functools import reduce

data_lines = reduce(lambda x, y: [x] + y if not isinstance(x, list) else (x + [y] if not isinstance(y, list) else x + y), data_lines)

len(data_lines)

from txtai.embeddings import Embeddings

# Create embeddings model, backed by sentence-transformers & transformers
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

quer = 'what is the use of Datascience in education?'

# Commented out IPython magic to ensure Python compatibility.
# %%time
# seq = embeddings.similarity(quer, data_lines)

three_most = seq[0:3]
three_most

indexes = []
for i in three_most:
  indexes.append(i[0])
  
indexes

for j in indexes:
    print(j, data_lines[j] )



"""## With webpages"""

import bs4 as bs
import urllib.request
import re

num_link = int(input("Enter number of Links - "))

# https://learnpython.com/blog/python-numpy-basics/
# https://www.nature.com/articles/s41586-020-2649-2

locations_max = []
for i in range (num_link):
  link = input("Enter the Required Link:  ")
  locations_max.append(link)

textract = Textractor(sentences=True)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data_lines = []
# article_text = " "
# for i in (locations_max):
#   print(i)
#   scraped_data = urllib.request.urlopen(i)
#   article = scraped_data.read()
#   parsed_article = bs.BeautifulSoup(article,'lxml')
#   paragraphs = parsed_article.find_all('p')
#   for p in paragraphs:
#     article_text += p.text
#   lines = textract(i)
#   data_lines.append(lines)

len(data_lines)

len(data_lines[0])

len(data_lines[1])

data_lines = reduce(lambda x, y: [x] + y if not isinstance(x, list) else (x + [y] if not isinstance(y, list) else x + y), data_lines)

len(data_lines)

from txtai.embeddings import Embeddings

# Create embeddings model, backed by sentence-transformers & transformers
embeddings = Embeddings({"path": "sentence-transformers/nli-mpnet-base-v2"})

quer = 'How to create array?'

# Commented out IPython magic to ensure Python compatibility.
# %%time
# seq = embeddings.similarity(quer, data_lines)

three_most = seq[0:3]
three_most

indexes = []
for i in three_most:
  indexes.append(i[0])
  
indexes

for j in indexes:
    print(j, data_lines[j] )